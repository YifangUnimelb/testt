---
title: "Forecasting Inflation and Interest Rate in Australia"
author: "Yifang"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This paper forecasts Australian inflation and interest rate using Bayesian Vector Regressive Model with stochastic volatility and a multivariate-t distributed error, with the purpose of allowing time varying volatility and capturing sudden increases in volatility during extreme period such as the COVID-19 pandemic. It is an application of such method, which is often considered as a significant improvement over a standard BVAR, to Australian macroeconomic variables to generate spot and density forecasts using post-COVID data.

> **Keywords.** BVARs, Stochastic Volatility, t-distributed error, inflation, interest rate, Australian economy

# Introduction

**Objective:** This paper aims to forecast Australian short term interest rate and inflation. It follows the method in Hartwig(2022), using a BVAR-SV model with its error covariance matrix being multivariate-t distribution.

**Question:** Can the model suggested produce reliable spot and density forecast for Australian inflation and interest rate?

**Motivation:** Volatility variation across time in macroeconomic variables is widely explored in literature (Bloom,2014), which the standard Gaussian error modeling may fail to capture and then leads to unreliable results. Hartwig(2022) suggests that several BVAR models with modified volatility distribution which differentiate from the standard Gaussian perform better under likelihood measurement. The best fitting one with post-COVID19 data is BVAR-t-SV. The stochastic process captures the time persistence of volatility, which is evident in variables such as short term interest rate(Ball and Torous,1999). A multivariate-t distributed error, with relatively fatter tails, can recognize some extreme volatility as temporary spikes instead of persistent effect. The forecast focus on inflation and interest rate due to their strong relevance to policy making and people's daily lives. Many challenges arise post-COVID for government to facilitate the revitalization of the economy, where forecast plays its roles. This research contributes to the existing literature by applying the forecasting method tested to fit the post-COVID data in other countries in Australian economy to facilitate better predictions.

# Data

All data is obtained from the RBA. Daily or monthly data is converted to quarterly format by averaging. Following Chan(2020), other than cash rate and unemployment, variables will be transformed using log difference times 400 in BVAR as a form of growth. These are some standard variables to include in a macroeconomics forecast:

**Cash rate:** the cash rate target in percent (series_id:FIRMMCRTD)

**Money aggregate(M1):** the seasonal adjusted M1 aggregate in \$billion (series_id:DMAM1S)

**Money aggregate(M3):** the seasonal adjusted M3 aggregate in \$billion (series_id:DMAM3S)

**Consumer Price Index** Consumer Price Index (series_id:GCPIAG)

**Real GDP:** Gross Domestic Product in real terms in \$million (series_id:GGDPCVGDP)

**Unemployment:** Unemployment rate in percent (series_id:GLFSUPSA)

The data window covers 1990 Q1 to 2023 Q4, as the most up-to-date post-COVID data in Australia, similar to Hartwig(2022). Cash rate, real GDP and unemployment rate are very standard variables to include while doing forecast with inflation as the cyclical variables(eg.Stock and Watson(1999)). Money aggregate as a supply side driver for commodity price, are included as well(eg. Dhakal et al(1994)).

```{r}
#| echo: false
#| message: false
#| warning: false

library(readrba)

un_em = read_rba(series_id = "GLFSURSA") 
gdp = read_rba(series_id = "GGDPCVGDP")
cpi = read_rba(series_id = "GCPIAG") 

cr = read_rba(series_id = "FIRMMCRTD")

m1 = read_rba(series_id = "DMAM1S") 
m3 = read_rba(series_id = "DMAM3S") 

library(dplyr)
library(lubridate)

cr <- cr %>% filter(date > as.Date("1989-12-31"))
cpi <- cpi %>% filter(date > as.Date("1989-12-31"))
gdp <- gdp %>% filter(date > as.Date("1989-12-31"))
m1 <- m1 %>% filter(date > as.Date("1989-12-31"))
m3 <- m3 %>% filter(date > as.Date("1989-12-31"))
un_em <- un_em %>% filter(date > as.Date("1989-12-31"))

cr <- cr %>% filter(date < as.Date("2024-01-01"))
cpi <- cpi %>% filter(date < as.Date("2024-01-01"))
gdp <- gdp %>% filter(date < as.Date("2024-01-01"))
m1 <- m1 %>% filter(date < as.Date("2024-01-01"))
m3 <- m3 %>% filter(date < as.Date("2024-01-01"))
un_em <- un_em %>% filter(date < as.Date("2024-01-01"))


m1 <- m1 %>%
  mutate(Quarter = paste(year(date), quarter(date), sep = "Q")) %>%
  group_by(Quarter) %>%
  summarize(Average = mean(value, na.rm = TRUE))

m3 <- m3 %>%
  mutate(Quarter = paste(year(date), quarter(date), sep = "Q")) %>%
  group_by(Quarter) %>%
  summarize(Average = mean(value, na.rm = TRUE))

un_em <- un_em %>%
  mutate(Quarter = paste(year(date), quarter(date), sep = "Q")) %>%
  group_by(Quarter) %>%
  summarize(Average = mean(value, na.rm = TRUE))

cr <- cr %>%
  mutate(Quarter = paste(year(date), quarter(date), sep = "Q")) %>%
  group_by(Quarter) %>%
  summarize(Average = mean(value, na.rm = TRUE))



```

```{r create data matrix}
#| echo: false
#| message: false
#| warning: false

library(dplyr)

# Assuming tibble1, tibble2, tibble3 are your tibbles
# and you want columnA from tibble1, columnB from tibble2
data <- bind_cols(
  cpi %>% select(date),
  cpi %>% select(value),
  cr %>% select(Average),
  gdp %>% select(value),
  m1 %>% select(Average),
  m3 %>% select(Average),
  un_em %>% select(Average)# Add as many as needed
)

data <- data %>% rename(cpi = value...2, cr = Average...3, gdp = value...4, m1 = Average...5, m3 = Average...6, un_em = Average...7)

data_trans <- data %>%
  mutate(cpi_log = c(log(cpi)), gdp_log =c(log(gdp)), m1_log =c(log(m1)), m3_log =c(log(m3)))

data_trans <- data_trans %>% select(date, cpi_log, gdp_log, m1_log, m3_log, cr, un_em)
```

## Time Series Plot

The following are the time series plot of raw CPI, real GDP, M1 and M3 monetary aggregate, unemployment rate and cash rate on quarterly basis. Note the former 4 variables all display a clear growth trend. Unemployment and cash rate are generally decreasing with fluctuations.

```{r times series raw}
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
library(patchwork)

theme_set(theme_bw())

pcpi <- ggplot(data, aes(date,cpi)) + geom_line(size = 0.25)+labs(title = "CPI") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

pgdp <- ggplot(data, aes(date,gdp)) + geom_line(size = 0.25)+labs(title = "GDP") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))


pm1 <- ggplot(data, aes(date,m1)) +geom_line(size = 0.25)+labs(title = "M1") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

pm3 <- ggplot(data, aes(date,m3)) + geom_line(size = 0.25)+labs(title = "M3") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

pcr <- ggplot(data, aes(date,cr)) + geom_line(size = 0.25)+labs(title = "cash rate") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

pun_em <- ggplot(data, aes(date,un_em)) + geom_line(size = 0.25)+labs(title = "unemployment rate") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

 
(pcpi + pgdp )/(pm1 + pm3 )/(pcr + pun_em)

```

The log level of CPI, real GDP, M1 and M3 aggregate were plotted here.

```{r time series log}
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
library(patchwork)

theme_set(theme_bw())

pcpil <- ggplot(data_trans, aes(date,cpi_log)) + geom_line(size = 0.25)+labs(title = "log CPI") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

pgdpl <- ggplot(data_trans, aes(date,gdp_log)) + geom_line(size = 0.25)+labs(title = "log GDP") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))


pm1l <- ggplot(data_trans, aes(date,m1_log)) +geom_line(size = 0.25)+labs(title = "log M1") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

pm3l <- ggplot(data_trans, aes(date,m3_log)) + geom_line(size = 0.25)+labs(title = "log M3") +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))
 
(pcpil + pgdpl )/(pm1l + pm3l )

```

## ACF and PACF

The following ACF plots of all transformed variables show autocorrelation over a large number of lags, therefore, likely to be non-stationary.

```{r ACF}
#| echo: false
#| message: false
#| warning: false

library(tidyr)

par(mfrow=c(2,3))

acf(data_trans$cpi_log)
acf(data_trans$gdp_log)
acf(data_trans$m1_log)
acf(data_trans$m3_log)
acf(data_trans$cr)
acf(data_trans$un_em)

```

In PACF plots, significant correlations were only observed in the first lag for variables except for unemployment rate. It demonstrates partial autocorrelation for the first and second lag, also in some lags order 15 to 20.

```{r PACF}
#| echo: false
#| message: false
#| warning: false

library(tidyr)

par(mfrow=c(2,3))

pacf(data_trans$cpi_log)
pacf(data_trans$gdp_log)
pacf(data_trans$m1_log)
pacf(data_trans$m3_log)
pacf(data_trans$cr)
pacf(data_trans$un_em)


```

## Augmented Dickey-Fuller

The table shows the result of Augmented Dickey-Fuller test. All time series are non-stationary except for cash rate in this test.

```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(tseries)
library(knitr)
library(kableExtra)

Y = data_trans |> 
  select(-date) |>
  ts(start = c(year(min(data_trans$date)), quarter(min(data_trans$date))), frequency = 4)

N = ncol(Y) 

p_value   = sapply(1:N, \(i) adf.test(Y[, i])$p.value)
variable  = colnames(Y)

adf       = cbind(variable, p_value) |> 
  data.frame() |> 
  mutate(p_value = round(as.numeric(p_value), 4)) |> 
  mutate(non_stationary = as.numeric(p_value > 0.05))

kable(adf, digits = 4)
```

The first difference ADF test only indicates non-stationarity for log m3 money supply.

```{r}
#| echo: false
#| message: false
#| warning: false
Y_diff    = diff(Y)
p_value   = sapply(1:N, \(i) adf.test(Y_diff[, i])$p.value)
variable  = colnames(Y)

cbind(variable, p_value) |> 
  data.frame() |> 
  mutate(p_value = round(as.numeric(p_value), 4)) |> 
  mutate(non_stationary = as.numeric(p_value > 0.05)) |> 
  kable(digits = 4)
```

All variables are integrated of order 2.

```{r}
#| echo: false
#| message: false
#| warning: false

Y_diff2    = diff(Y_diff)
p_value   = sapply(1:N, \(i) adf.test(Y_diff2[, i])$p.value)
variable  = colnames(Y)

cbind(variable, p_value) |> 
  data.frame() |> 
  mutate(p_value = round(as.numeric(p_value), 4)) |> 
  mutate(non_stationary = as.numeric(p_value > 0.05)) |> 
  kable(digits = 4)
```

#Model

Here presents the Bayesian VAR in the general form of a VAR(p):

$$y_t = a_0 +A_1y_{t-1}+...+A_py_{t-p}+ \epsilon_t $$

where

-   $y_t$ is a $n \times 1$ vector
-   $n$ is the number of variables
-   $a_0$ is the $n \times 1$ intercept vector
-   $A_p$ is $n \times n$ coefficient matrix for each lag order $p$.

In compact matrix notation:

$$Y = XA+E$$ where

-   $Y_{T \times n}=(y_1', y_2',...,y_T')'$
-   $X_{T \times K}=(x_1', x_2',...,x_T')'$
-   $x_{i K \times 1}=(1, y_{t-1}',...,y_{t-p}')'$ for i = 1,...T.
-   $A_{K \times n}=(a_0', A_1',...,A_p')'$ is a compact coefficient matrix
-   $E_{T \times n}=(\epsilon_1', \epsilon_2',...,\epsilon_T')'$.
-   $K = 1+pn$.

In standard form, the error term is normally distributed $$\epsilon_t \sim iidN(0, \Sigma)$$ or $$vec(E) \sim N(0, \Sigma \otimes I_T)$$

-   $\Sigma$ is a $n \times n$ covariance matrix
-   $I_T$ is an $T \times T$ identity matrix.

To allow for non-Gaussian error, we can relax the assumption for the identity matrix and substitute it for $\Omega$:

$$vec(E) \sim N(0,\Sigma \otimes \Omega)$$\

The specifications of $\Omega$ and their meaning will be discussed in the next session.

# Estimation Procedure

## The Baseline Model

# References
