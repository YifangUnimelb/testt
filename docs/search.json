[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "",
    "text": "Abstract. This paper forecasts Australian inflation and interest rate using Bayesian Vector Auto Regressive Model with extended features such as stochastic volatility and multivariate-t distributed error term, with the purpose of allowing time varying volatility and the error to capture sudden increases in volatility during periods with extreme events such as the COVID-19 pandemic. It is an application of such method, which is often considered as a significant improvement over a standard BVAR, to Australian macroeconomic variables to generate spot and density forecasts using post-COVID data."
  },
  {
    "objectID": "index.html#time-series-plots",
    "href": "index.html#time-series-plots",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "Time Series Plots",
    "text": "Time Series Plots\nThe following are the time series plot all variables."
  },
  {
    "objectID": "index.html#autocorrelation-function-acf-and-partial-autocorrelation-function-pacf",
    "href": "index.html#autocorrelation-function-acf-and-partial-autocorrelation-function-pacf",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)",
    "text": "Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\nThe following ACF plots of all transformed variables show autocorrelation over a large number of lags, therefore, likely to be non-stationary.\n\n\n\n\n\n\n\n\n\nIn PACF plots, significant correlations were only observed in the first lag for variables except for unemployment rate. It demonstrates partial autocorrelation for the first and second lag, also in some lags order 15 to 20."
  },
  {
    "objectID": "index.html#augmented-dickey-fuller-test",
    "href": "index.html#augmented-dickey-fuller-test",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "Augmented Dickey Fuller Test",
    "text": "Augmented Dickey Fuller Test\nThe table shows the result of Augmented Dickey-Fuller test. All time series are non-stationary at 5% significance level except for cash rate in this test.\n\n\n\n\n\nvariable\np_value\nnon_stationary\n\n\n\n\ncpi_log\n0.1161\n1\n\n\ngdp_log\n0.8745\n1\n\n\nm1_log\n0.1178\n1\n\n\nex\n0.6245\n1\n\n\ncr\n0.0107\n0\n\n\nun_em\n0.3862\n1\n\n\n\n\n\nThe table shows the result of Augmented Dickey-Fuller test using the first differenced data. All variables are integrated of order 1.\n\n\n\n\n\nvariable\np_value\nnon_stationary\n\n\n\n\ncpi_log\n0.0180\n0\n\n\ngdp_log\n0.0100\n0\n\n\nm1_log\n0.0219\n0\n\n\nex\n0.0100\n0\n\n\ncr\n0.0177\n0\n\n\nun_em\n0.0100\n0"
  },
  {
    "objectID": "index.html#the-baseline-model",
    "href": "index.html#the-baseline-model",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "The Baseline Model",
    "text": "The Baseline Model\nThe baseline model is the standard form where \\(\\Omega\\) = \\(I_T\\). With predictive density:\n\\[Y|X,A,\\Sigma \\sim MN_{T \\times N} (XA, \\Sigma, I_T)\\] Therefore the kernel of the likelihood function: \\[L(A,\\Sigma|Y,X) \\propto det(\\Sigma)^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)])\\] The matrix-variate normal and inverse Wishart natural conjugate priors for \\(A\\) and \\(\\Sigma\\):\n\\[p(A,\\Sigma) = p(A|\\Sigma)p(\\Sigma)\\] \\[A|\\Sigma = MN_{K\\times N}(\\underline{A},\\Sigma,\\underline{V})\\] \\[\\Sigma \\sim IW_N(\\underline{S},\\underline{\\nu})\\] Therefore the kernel of the prior distribution:\n\\[\n\\begin{align}\np(A,\\Sigma) &\\propto  \\det(\\Sigma)^{-\\frac{N+K+\\underline{v}+1}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A}) \\underline{V}^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\}\n\\end{align}\n\\]\nDerive the product of the likelihood function and the density function \\(p(A,\\Sigma)\\):\n\\[p(A,\\Sigma|Y,X) \\propto L(A,\\Sigma|Y,X)p(A,\\Sigma) \\] Collect corresponding terms and then we have the full conditional posterior distribution:\n\\[A|\\Sigma = MN_{K\\times N}(\\bar{A},\\Sigma,\\bar{V})\\] \\[\\Sigma \\sim IW_N(\\bar{S},\\bar{\\nu})\\] where the parameters are:\n\\[\\bar{V} = (X'X + \\underline{V}^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X'Y + \\underline{V}^{-1}\\underline{A})\\] \\[\\bar{S} = \\underline{S}+Y'Y + \\underline{A}'\\underline{V}^{-1}\\underline{A}-\\bar{A}'\\bar{V}^{-1}\\bar{A}\\] \\[\\bar{\\nu}= T + \\underline{\\nu}\\] To accommodate non-stationary data, we use minnesota prior:\n\\[\\underline{A} = [0_{N\\times1}   \\space\\space  I_N    \\space\\space 0_{N \\times (p-1N) }]'\\] \\[\\underline{V} = diag([\\kappa_{2} \\space \\space  \\kappa_{1} (p^{-2} \\otimes i_{N}')])\\] This prior assumes the macroeconomic time-series leans towards a random walk given that except for first lag, all other slope parameters in \\(\\underline{A}\\) are zero. The shrinkage parameters in \\(\\underline{V}\\) are set to be \\(\\kappa_{2}\\), a relatively large value as there is little information about the intercept parameters. And for slope parameters, \\(\\kappa_{1}\\) will be significantly smaller and decreasing over lags.\nThe following function represents the estimation procedure:\n\n#Function form of baseline model\n\nbvar_est = function(Y,X,p){\n  \n  #set parameters\n  t = nrow(Y)\n  N = ncol(Y) #number of variables\n  S = 10000 #number of draws\n  \n  #mle\n  A.hat  = solve(t(X)%*%X)%*%t(X)%*%Y\n  Sigma.hat  = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/t\n  \n  # minnesota prior\n  k1     = 5\n  k2     = 100\n  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n  A.prior[2:(1+N),] = diag(N)\n  V.prior     = diag(c(k2,k1*((1:p)^(-2))%x%rep(1,N)))\n  S.prior     = diag(diag(Sigma.hat))\n  nu.prior    = N+1\n  \n  # NIW posterior\n\n  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))\n  V.bar       = solve(V.bar.inv)\n  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\n  nu.bar      = nrow(Y) + nu.prior\n  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar  \n  S.bar.inv   = solve(S.bar)\n  \n  #draws from posterior\n\n  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)\n  Sigma.posterior   = apply(Sigma.posterior,3,solve)\n  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))\n  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))\n  L                 = t(chol(V.bar))\n\n  for (s in 1:S){\n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n  }\n\n  round(apply(A.posterior,1:2,mean),3)\n\n  # report posterior means and sd of parameters\n  A.E         = apply(A.posterior,1:2,mean)\n  Sigma.E     = apply(Sigma.posterior,1:2,mean)\n\n  output = list(\n    A.E = A.E,\n    Sigma.E = Sigma.E,\n    A.posterior.draws = A.posterior,\n    Sigma.posterior.draws = Sigma.posterior\n  )\n  return(output)\n}\n\n\nPosterior Draws\nMean matrix \\(A\\):\n\nhead(round(apply(baseline.posterior.draws$A.posterior.draws,1:2,base::mean),6))\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,]  0.030626  0.069606 -0.045241  0.196977  2.089513 -1.160189\n[2,]  0.991707 -0.016549  0.011426 -0.037019 -0.117729  0.711609\n[3,]  0.000249  1.000479  0.003573  0.005744  0.035307 -0.277785\n[4,]  0.004468  0.000066  1.005247  0.003535 -0.099197 -0.051403\n[5,] -0.002675  0.009302 -0.034879  1.022107  0.309952 -0.337139\n[6,]  0.005276 -0.002776 -0.015232 -0.000551  1.559531 -0.149462\n\n\nMean matrix \\(\\Sigma\\):\n\nhead(round(apply(baseline.posterior.draws$Sigma.posterior.draws,1:2,base::mean),6))\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,]  0.000028  0.000016 -0.000016  0.000030  0.000369 -0.000487\n[2,]  0.000016  0.000092 -0.000022  0.000051  0.000537 -0.001232\n[3,] -0.000016 -0.000022  0.000525 -0.000105 -0.002380  0.000600\n[4,]  0.000030  0.000051 -0.000105  0.001193  0.004169 -0.001237\n[5,]  0.000369  0.000537 -0.002380  0.004169  0.082604 -0.015063\n[6,] -0.000487 -0.001232  0.000600 -0.001237 -0.015063  0.055229\n\n\n\n\nTrace Plot\nThe following diagrams are the trace plots and histograms of the first lag auto-correlative parameters for variable \\(CPI \\underline{} log_t\\) in the \\(A\\) matrices in the posterior draws. The autocorrelation is significant while the variance is quite small.\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\n\nplot.ts(baseline.posterior.draws$A.posterior.draws[2,1,], xlab = \"Simulation times S\", ylab = \"CPI\", col = mcxs1)\nhist(baseline.posterior.draws$A.posterior.draws[2,1,], xlab = \"CPI\", col = mcxs1, main = '')\n\nplot.ts(baseline.posterior.draws$Sigma.posterior.draws[1,1,], xlab = \"Simulation times S\", ylab = \"CPI sigma\", col = mcxs2)\nhist(baseline.posterior.draws$Sigma.posterior.draws[1,1,], xlab = \"CPI sigma\", col = mcxs2, main = '')\n\n\n\n\n\n\n\n\nSimilarly, for \\(CR_t\\), the autocorrelation is significant. Its variance is visibly larger than the previous variable.\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\n\nplot.ts(baseline.posterior.draws$A.posterior.draws[6,5,], xlab = \"Simulation times S\", ylab = \"Cash Rate\", col = mcxs1)\nhist(baseline.posterior.draws$A.posterior.draws[6,5,], xlab = \"Cash Rate\", col = mcxs1, main = '')\n\nplot.ts(baseline.posterior.draws$Sigma.posterior.draws[5,5,], xlab = \"Simulation times S\", ylab = \"Cash Rate sigma\", col = mcxs2)\nhist(baseline.posterior.draws$Sigma.posterior.draws[5,5,], xlab = \"Cash Rate sigma\", col = mcxs2, main = '')"
  },
  {
    "objectID": "index.html#the-extended-model-1---student-t-distributed-error-bvar-t",
    "href": "index.html#the-extended-model-1---student-t-distributed-error-bvar-t",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "The Extended Model 1 - Student-t Distributed Error (BVAR-T)",
    "text": "The Extended Model 1 - Student-t Distributed Error (BVAR-T)\nThe model is where \\(\\Omega\\) = \\(diag(\\lambda_1, \\lambda_2,...,\\lambda_T)\\). Each lambda independently drawn from an Inverse Gamma 2 distribution \\(IG2(\\nu-2,\\nu)\\). This implies the distribution of error term being:\n\\[ \\epsilon_t \\sim N(0, \\lambda_t \\Sigma)\\]\ncompared to the baseline model, the error distribution has fatter tails which captures sudden spike of variance.The predictive density:\n\\[Y|X,A,\\Sigma \\sim MN_{T \\times N} (XA, \\Sigma, \\Omega)\\] Therefore the kernel of the likelihood function:\n\\[L(A,\\Sigma,\\Omega|Y,X) \\propto det(\\Omega)^{-\\frac{N}{2}}det(\\Sigma)^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'\\Omega^{-1}(Y-XA)])\\] I assume the same matrix-variate normal and inverse Wishart natural conjugate priors for \\(A\\) and \\(\\Sigma\\). Derive the product of the likelihood function and the density function \\(p(A,\\Sigma)\\), collect corresponding terms and then we have the full conditional posterior distribution:\n\\[A|\\Sigma = MN_{K\\times N}(\\bar{A},\\Sigma,\\bar{V})\\] \\[\\Sigma \\sim IW_N(\\bar{S},\\bar{\\nu})\\] where the parameters are:\n\\[\\bar{V} = (X'\\Omega^{-1}X + \\underline{V}^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X'\\Omega^{-1}Y + \\underline{V}^{-1}\\underline{A})\\] \\[\\bar{S} = \\underline{S}+Y'\\Omega^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A}-\\bar{A}'\\bar{V}^{-1}\\bar{A}\\] \\[\\bar{\\nu}= T + \\underline{\\nu}\\] To derive the full conditional posterior for \\(\\lambda_t\\), the kernel of likelihood function can be expressed as:\n\\[L(A,\\Sigma,\\Omega|Y,X) \\propto det(\\Omega)^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'\\Omega^{-1}(Y-XA)])\\] \\[= (\\prod^{T}_{i = 1} \\lambda_t^{-\\frac{N}{2}})exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\sum^{T}_{i =1}\\epsilon_t' \\Sigma^{-1}\\epsilon_t)\\] That is for each \\(t\\), the likelihood function is:\n\\[ \\lambda_t^{-\\frac{N}{2}}exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t)\\] The density function for \\(\\lambda_t \\sim IG2(\\nu_{\\lambda}-2,\\nu_{\\lambda})\\) is:\n\\[\\lambda_t^{-\\frac{\\nu_{\\lambda}-2+2}{2}}exp({-\\frac{1}{2}}{\\frac{\\nu_{\\lambda}}{\\lambda_t}})\\] The posterior density for \\(\\lambda_t\\) is:\n\\[p(\\lambda_t|Y,X,A,\\Sigma,\\nu_{\\lambda})= \\lambda_t^{-\\frac{N+\\nu_{\\lambda}+2}{2}}exp[{-\\frac{1}{2}}{\\frac{1}{\\lambda_t}}(\\nu_{\\lambda}-2+\\epsilon_t' \\Sigma^{-1}\\epsilon_t))]\\] Where \\[\\epsilon_t = (y_t- x_t'A).\\]\nThe posterior follows an \\(IG2\\) distribution:\n\\[\\lambda_t|Y,X,A,\\Sigma,\\nu_{\\lambda} \\sim IG2(\\bar\\nu_{\\lambda},\\bar{s}_{\\lambda})\\] \\[\\bar\\nu_{\\lambda} = N+\\underline\\nu_{\\lambda}\\] \\[\\bar{s}_{\\lambda} =\\underline\\nu_{\\lambda}-2+\\epsilon_t' \\Sigma^{-1}\\epsilon_t\\] Here, we need a gibb sampler to estimate each parameter. I initialize a \\(\\Omega^{(0)}\\), which is a diagonal matrix with each element \\(\\lambda_t\\) drawn from \\(IG2(\\underline\\nu_{\\lambda}-2,\\underline\\nu_{\\lambda})\\) and let \\(\\bar\\nu_{\\lambda}=5\\).Then:\n\nDraw \\(\\Sigma^{(s)}\\) from \\(IW_N(\\bar{S},\\bar{\\nu})\\).\nDraw \\(A^{(s)}\\) from \\(MN_{K\\times N}(\\bar{A},\\Sigma^{(s)},\\bar{V})\\).\n\nThen for each iteration \\(s\\), I draw \\(T\\) \\(\\lambda_t\\) using corresponding row in the error matrix:\n\nDraw \\(\\lambda_t^{(s)}\\) from \\(IG2(\\bar\\nu_{\\lambda},\\bar{s}_{\\lambda})\\).\n\nThen form \\(\\Omega^{(s)}\\) with \\(\\lambda_t^{(s)}\\), update all the priors, repeat.\nThe following function represents the estimation procedure:\n\n#Function form of BVAR_t model\n\nbvar_t = function(Y,X,p){\n\n# Set parameters\nS =10000\nN = ncol(Y)\nt &lt;- nrow(Y)  \n\n#mle\n\nA.hat  = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat  = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/t\n\n#minnesota\n\nk1     = 5\nk2     = 100\nA.prior &lt;- matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(1+N),] = diag(N)\nV.prior     = diag(c(k2,k1*((1:p)^(-2))%x%rep(1,N)))\nSigma.s.prior    = diag(diag(Sigma.hat))\nSigma.v.prior    = N+1\nlambda.nu.prior = 5\n\n#inital lambda/omega\n\nlambda0 = (lambda.nu.prior-2)/rchisq(t,lambda.nu.prior)\n\n# Initialize arrays to store posterior draws\n  Sigma.posterior.draws = array(NA, c(N,N,S))\n  A.posterior.draws = array(NA, c((1+p*N),N,S))\n  \n  lambda.posterior.draws = array(NA,c(t,S+1))\n  lambda.posterior.draws[,1] = lambda0 \n  lambda.s.posterior = array(NA,c(t,S))\n  \nfor (s in 1:S){ \n    \n    V.posterior     = solve(t(X)%*%diag(1/(lambda.posterior.draws[,s]))%*%X + solve(V.prior))\n    \n    A.posterior     = V.posterior%*%(t(X)%*%diag(1/(lambda.posterior.draws[,s]))%*%Y + solve(V.prior)%*%A.prior)\n    \n    Sigma.s.posterior = t(Y)%*%diag(1/(lambda.posterior.draws[,s]))%*%Y + t(A.prior)%*%solve(V.prior)%*%A.prior + Sigma.s.prior - t(A.posterior)%*%solve(V.posterior)%*%A.posterior\n    \n    Sigma.v.posterior = nrow(Y) + Sigma.v.prior\n    \n    Sigma.posterior.draws[,,s] = riwish(Sigma.v.posterior, Sigma.s.posterior)\n\n    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.posterior), sigma=Sigma.posterior.draws[,,s]%x%V.posterior), ncol=N)\n    \n    E.s = Y-X%*%A.posterior.draws[,,s]\n    \n    for (x in 1:t){\n    lambda.s.posterior[,s] = lambda.nu.prior-2 + t((E.s)[x,])%*%Sigma.posterior.draws[,,s]%*%(E.s)[x,]\n    }\n    \n    lambda.v.posterior = N + lambda.nu.prior\n    \n    for (x in 1:t){\n    lambda.posterior.draws[x,s+1] = lambda.s.posterior[x,s]/rchisq(1,lambda.v.posterior)\n    }\n    \n} \n  output = list(\n    A.posterior.draws = A.posterior.draws,\n    Sigma.posterior.draws = Sigma.posterior.draws,\n    lambda.posterior.draws = lambda.posterior.draws\n  )\n  return(output)\n}\n\n\nPosterior draws\nMean matrix \\(A\\):\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,]  0.080256  0.193781 -0.174144  0.509220  5.647447 -2.557839\n[2,]  0.980399 -0.039892  0.030185 -0.075348 -0.038113  1.785887\n[3,] -0.000425  0.995506 -0.001398 -0.004922 -0.127035 -0.746150\n[4,]  0.010239  0.003480  1.026719  0.013330 -0.047067 -0.180517\n[5,] -0.002548  0.017454 -0.054412  1.091824  0.722904 -0.632265\n[6,]  0.005847 -0.003977 -0.013280 -0.003554  1.597555 -0.156624\n\n\nMean matrix \\(\\Sigma\\):\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,]  0.000096  0.000054 -0.000055  0.000100  0.001230 -0.001661\n[2,]  0.000054  0.000317 -0.000078  0.000162  0.001957 -0.004268\n[3,] -0.000055 -0.000078  0.001831 -0.000352 -0.008120  0.001899\n[4,]  0.000100  0.000162 -0.000352  0.004133  0.014024 -0.003685\n[5,]  0.001230  0.001957 -0.008120  0.014024  0.275730 -0.047804\n[6,] -0.001661 -0.004268  0.001899 -0.003685 -0.047804  0.184662\n\n\n\n\nTrace Plot\nThe following diagrams are the trace plots and histograms of the first lag auto-correlative parameters and variances for variable \\(CPI \\underline{} log_t\\) and \\(CR \\underline{} log_t\\) in the posterior draws.There characteristics are similar to the baseline model.\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\n\nplot.ts(bvart$A.posterior.draws[2,1,], xlab = \"Simulation times S\", ylab = \"CPI\", col = mcxs1)\nhist(bvart$A.posterior.draws[2,1,], xlab = \"CPI\", col = mcxs1, main = '')\n\nplot.ts(bvart$Sigma.posterior.draws[1,1,], xlab = \"Simulation times S\", ylab = \"CPI sigma\", col = mcxs2)\nhist(bvart$Sigma.posterior.draws[1,1,], xlab = \"CPI sigma\", col = mcxs2, main = '')\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\n\nplot.ts(bvart$A.posterior.draws[6,5,], xlab = \"Simulation times S\", ylab = \"Cash Rate\", col = mcxs1)\nhist(bvart$A.posterior.draws[6,5,], xlab = \"Cash Rate\", col = mcxs1, main = '')\n\nplot.ts(bvart$Sigma.posterior.draws[5,5,], xlab = \"Simulation times S\", ylab = \"Cash Rate sigma\", col = mcxs2)\nhist(bvart$Sigma.posterior.draws[5,5,], xlab = \"Cash Rate sigma\", col = mcxs2, main = '')"
  },
  {
    "objectID": "index.html#the-extended-model-2---stochastic-volatility-bvar-sv",
    "href": "index.html#the-extended-model-2---stochastic-volatility-bvar-sv",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "The Extended Model 2 - Stochastic Volatility (BVAR-SV)",
    "text": "The Extended Model 2 - Stochastic Volatility (BVAR-SV)\nFor this model, the \\(\\Omega = diag(e^{h1},e^{h2},...,e^{h_T})\\), where \\(h_t\\) follows an AR(1) process:\n\\[h_t = h_{t-1}+\\sigma_v v_t\\] where \\(v_{t} \\sim N(0, 1)\\) and \\(\\sigma^{2}_v \\sim IG2(\\bar{s}, \\bar{v})\\), which implies: \\[\\epsilon_{t} \\sim N(0,e^{h_t} \\Sigma)\\] Under this specification, \\(A\\) and \\(\\Sigma)\\) have a similar full conditional posterior distribution to the BVAR-T model, only with \\(\\Omega\\) modified.\n\n##Function form of BVAR-SV model \n\nbvar_sv = function (Y, X, p){\n  \n  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\n  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n  \n  N = ncol(Y)\n  K = ncol(X)\n  T = nrow(Y)\n\n  kappa.1   = 5\n  kappa.2   = 100\n  \n  A.prior     = matrix(0, K , N)\n  A.prior[2:(N+1),] = diag(N)\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n  V.prior.inv = diag(1/diag(V.prior))\n  S.prior     = diag(diag(Sigma.hat))\n  nu.prior    = N+2\n\n  S = 10000\n  \n  # aux is a list containing:\n  #   Y - a TxN matrix\n  #   X - a TxK matrix\n  #   H - a Tx1 matrix\n  #   h0 - a scalar\n  #   sigma.v2 - a scalar\n  #   s - a Tx1 matrix\n  #   A - a KxN matrix\n  #   Sigma - an NxN matrix\n  #   sigma2 - a Tx1 matrix\n  #\n  # priors is a list containing:\n  #   h0.v - a positive scalar\n  #   h0.m - a scalar\n  #   sigmav.s - a positive scalar\n  #   sigmav.nu - a positive scalar\n  #   HH - a TxT matrix\n  \n  T             = dim(Y)[1]\n  N             = dim(Y)[2]\n  K             = dim(X)[2] \n\n  H             = diag(T)\n  sdiag(H,-1)   = -1\n  HH            = 2*diag(T)\n  sdiag(HH,-1)  = -1\n  sdiag(HH,1)   = -1\n  \n  HH                = HH\n  h0.m              = 0\n  h0.v              = 1\n  sigmav.s          = 1\n  sigmav.nu         = 1\n\n  posteriors    = list(                        \n    H           = matrix(NA,T,S),\n    sigma2      = matrix(NA,T,S),\n    s           = matrix(NA,T,S),\n    h0          = rep(NA,S),\n    sigma.v2    = rep(NA,S),\n    A           = array(NA, c((1 + N * p), N, S)),\n    Sigma       = array(NA, c(N, N, S))\n  )\n  \n  aux        = list(\n    Y        = Y,\n    X        = X,\n    H        = matrix(1, T, 1),\n    h0       = 0,\n    sigma.v2 = 1,\n    s        = matrix(1, T, 1),\n    A        = matrix(0, K, N),\n    Sigma    = matrix(0, N, N),             \n    sigma2   = matrix(1, T, 1)\n  )\n  \n  for (s in 1:S){\n      # normal-inverse Wishard posterior \n      V.bar.inv              = t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$X + diag(1/ diag(V.prior))\n      V.bar                  = solve(V.bar.inv)\n      A.bar                  = V.bar%*%(t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y + diag(1/diag( V.prior))%*%A.prior) \n      nu.bar                 = T + nu.prior\n      S.bar                  = S.prior + t(aux$Y)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar \n      S.bar.inv              = solve(S.bar)\n      S.bar.inv              = 0.5 * (t(S.bar.inv) + S.bar.inv) #positive-definite\n    \n      # posterior draws for A and Sigma\n      Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n      Sigma.posterior.draw   = apply(Sigma.posterior.IW, 3 ,solve)\n      aux$Sigma              = array(Sigma.posterior.draw,c(N,N,1))\n      A.norm                 = array(rnorm(prod(c(K,N,1))),c(K,N,1))\n      L                      = t(chol(V.bar))\n      aux$A                  = A.bar + L%*%A.norm[,,1]%*%chol(aux$Sigma[,,1])\n  \n      # posterior draw for sigma2   \n      N             = dim(aux$Y)[2] # 10\n      alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)\n      sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)\n      pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)\n      \n      Lambda        = solve(chol(aux$Sigma[,,1]))\n      Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)\n      Y.tilde       = as.vector(log((Z + 0.0000001)^2))\n      Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])\n        \n      # sampling initial condition\n      ############################################################\n      V.h0.bar      = 1/((1 / h0.v) + (1 / aux$sigma.v2))\n      m.h0.bar      = V.h0.bar*((h0.m / h0.v) + (aux$H[1] / aux$sigma.v2))\n      h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))\n      aux$h0        = h0.draw\n      \n      # sampling sigma.v2\n      ############################################################\n      sigma.v2.s    = sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)\n      sigma.v2.draw = sigma.v2.s / rchisq(1, sigmav.nu + T)\n      aux$sigma.v2  = sigma.v2.draw\n      \n      # sampling auxiliary states\n      ############################################################\n      Pr.tmp        = simplify2array(lapply(1:10,function(x){\n        dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])\n      }))\n      Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))\n      s.cum         = t(apply(Pr, 1, cumsum))\n      r             = matrix(rep(runif(T), 10), ncol = 10)\n      ss            = apply(s.cum &lt; r, 1, sum) + 1\n      aux$s         = as.matrix(ss)\n      \n      # sampling log-volatilities using functions for tridiagonal precision matrix\n      ############################################################\n      Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])\n      D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * HH\n      b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])\n      lead.diag     = diag(D.inv)\n      sub.diag      = mgcv::sdiag(D.inv, -1)\n      D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)\n      D.L           = diag(D.chol$ld)\n      mgcv::sdiag(D.L,-1) = D.chol$sd\n      x             = as.matrix(rnorm(T))\n      a             = forwardsolve(D.L, b)\n      draw          = backsolve(t(D.L), a + x)\n      aux$H         = as.matrix(draw)\n      aux$sigma2    = as.matrix(exp(draw))\n        \n      # output list\n      posteriors$H[,s]             = aux$H\n      posteriors$sigma2[,s]        = aux$sigma2\n      posteriors$s[,s]             = aux$s\n      posteriors$h0[s]             = aux$h0\n      posteriors$sigma.v2[s]       = aux$sigma.v2\n      posteriors$A[,,s]            = aux$A\n      posteriors$Sigma[,,s]        = aux$Sigma\n      \n  }\n  return(posteriors)\n}\n\n\nPosterior draws\nMean matrix \\(A\\):\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,]  0.002283  0.003327  0.002904  0.007714  0.215520 -0.179313\n[2,]  0.999699 -0.000161  0.000226 -0.002292 -0.015122  0.045113\n[3,]  0.000625  1.001280  0.002289  0.002328  0.055003 -0.056923\n[4,] -0.000229 -0.002201  0.997678 -0.001372 -0.073816  0.076196\n[5,]  0.000127 -0.000023 -0.001808  0.995498  0.027719 -0.009272\n[6,]  0.001836 -0.000285 -0.007554  0.000437  1.186747 -0.043034\n\n\nMean matrix \\(\\sigma\\):\n\n\n         [,1]     [,2]     [,3]     [,4]      [,5]      [,6]\n[1,]  1.0e-06  0.0e+00 -1.0e-06  0.0e+00  0.000008 -0.000013\n[2,]  0.0e+00  2.0e-06  0.0e+00  0.0e+00  0.000005 -0.000018\n[3,] -1.0e-06  0.0e+00  1.4e-05 -2.0e-06 -0.000071  0.000011\n[4,]  0.0e+00  0.0e+00 -2.0e-06  4.7e-05  0.000014 -0.000077\n[5,]  8.0e-06  5.0e-06 -7.1e-05  1.4e-05  0.003330 -0.000791\n[6,] -1.3e-05 -1.8e-05  1.1e-05 -7.7e-05 -0.000791  0.002300\n\n\n\n\nTrace Plot\nThe following diagrams are the trace plots and histograms of the first lag auto-correlative parameters for \\(CPI \\underline{} log_t\\) and \\(CR \\underline{} log_t\\) in the \\(A\\) matrices in the posterior draws. Also the variance elements in \\(\\Sigma\\).\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\n\nplot.ts(A.posterior.sv[2,1,], xlab = \"Simulation times S\", ylab = \"CPI\", col = mcxs1)\nhist(A.posterior.sv[2,1,], xlab = \"CPI\", col = mcxs1, main = '')\n\nplot.ts(Sigma.posterior.sv[1,1,], xlab = \"Simulation times S\", ylab = \"CPI sigma\", col = mcxs2)\nhist(Sigma.posterior.sv[1,1,], xlab = \"CPI sigma\", col = mcxs2, main = '')\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2), mar=c(4,4,2,2))\n\nplot.ts(A.posterior.sv[6,5,], xlab = \"Simulation times S\", ylab = \"Cash Rate\", col = mcxs1)\nhist(A.posterior.sv[6,5,], xlab = \"Cash Rate\", col = mcxs1, main = '')\n\nplot.ts(Sigma.posterior.sv[5,5,], xlab = \"Simulation times S\", ylab = \"Cash Rate sigma\", col = mcxs2)\nhist(Sigma.posterior.sv[5,5,], xlab = \"Cash Rate sigma\", col = mcxs2, main = '')"
  },
  {
    "objectID": "index.html#the-extended-model-3---t-distributed-error-and-stochastic-volatility-bvar-t-sv",
    "href": "index.html#the-extended-model-3---t-distributed-error-and-stochastic-volatility-bvar-t-sv",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "The Extended Model 3 - T-distributed error and Stochastic Volatility (BVAR-T-SV)",
    "text": "The Extended Model 3 - T-distributed error and Stochastic Volatility (BVAR-T-SV)\nCombining the previous 2 specification,the \\(\\Omega = diag(e^{h1}\\lambda_1,e^{h2}\\lambda_2,...,e^{h_T}\\lambda_T)\\) and it implies:\n\\[\\epsilon_t \\sim N(0, e^{t}\\lambda_t\\Sigma)\\] In this case, the likelihood function is relevant to \\(\\lambda_t\\) and \\(h_t\\):\n\\[L(A,\\Sigma,\\Omega|Y,X) \\propto det(\\Omega)^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'\\Omega^{-1}(Y-XA)])\\]\n\\[= (\\prod^{T}_{i = 1} (e^{h_t}\\lambda_t)^{-\\frac{N}{2}})exp({-\\frac{1}{2}}{\\frac{1}{e^{h_t}\\lambda_t}} \\sum^{T}_{i =1}\\epsilon_t' \\Sigma^{-1}\\epsilon_t)\\] That is for each \\(t\\), the likelihood function is:\n\\[ e^{h_t}\\lambda_t^{-\\frac{N}{2}}exp({-\\frac{1}{2}}{\\frac{1}{e^{h_t}\\lambda_t}} \\epsilon_t' \\Sigma^{-1}\\epsilon_t)\\]"
  },
  {
    "objectID": "index.html#computational-validity",
    "href": "index.html#computational-validity",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "Computational Validity",
    "text": "Computational Validity\nThe following contains code for the estimation and a results demonstration using a bivariate Gaussian random walk data, with true parameter \\(A = [0_{2 \\times 1} \\space I_2]'\\) and \\(\\Sigma = I_2\\), to demonstrate the computational validity of this method.\n\n\n\n\n\n\n\n\n\nEstimates from Standard BVAR Algorithm:\n\nrw = bvar_est(Y_rw, X_rw, p_rw)\n\n# A posterior mean\nround(rw$A.E,2)\n\n     [,1]  [,2]\n[1,] 0.28  0.39\n[2,] 0.91 -0.08\n[3,] 0.02  0.98\n\nround(rw$Sigma.E,2)\n\n     [,1] [,2]\n[1,] 0.92 0.16\n[2,] 0.16 0.93\n\n\nEstimates from BVAR-T Algorithm:\n\nrw_t = bvar_t(Y_rw, X_rw, p_rw)\nlambda0 = (lambda.nu.prior-2)/rchisq(100,lambda.nu.prior)\n\n# A posterior mean\napply(rw_t$A.posterior.draws, 1:2, base::mean)\n\n           [,1]        [,2]\n[1,] 0.28591444  0.39142107\n[2,] 0.91046535 -0.08412132\n[3,] 0.01879917  0.98342631\n\n# Sigma posterior mean\napply(rw_t$Sigma.posterior.draws, 1:2, base::mean)*mean(lambda0)\n\n          [,1]      [,2]\n[1,] 1.8648432 0.3330794\n[2,] 0.3330794 1.8938892\n\n\nEstimates from BVAR-SV Algorithm:\n\nrw_sv = bvar_sv(Y_rw, X_rw, p_rw)\n\n# A posterior mean\napply(rw_sv$A, 1:2, base::mean)\n\n            [,1]        [,2]\n[1,] 0.423240400  0.36801120\n[2,] 0.916210044 -0.03567557\n[3,] 0.004136077  0.96499089\n\n# Sigma posterior mean\napply(rw_sv$Sigma, 1:2, base::mean)*mean(rw_sv$sigma2)\n\n           [,1]       [,2]\n[1,] 107.726195  -9.898669\n[2,]  -9.898669 104.697345\n\n\nWe can see all three methods provide rather reliable estimates for \\(A\\) and thus the point forecast will have good correspondance to data."
  },
  {
    "objectID": "index.html#baseline-model-vs.-t-distributed-error",
    "href": "index.html#baseline-model-vs.-t-distributed-error",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "Baseline Model vs. T-distributed Error",
    "text": "Baseline Model vs. T-distributed Error\nThe point forecasts for both variables are very similar from baseline model and BVAR-T. While the 68% predictive density interval from BVAR-T is visibly wider due to the fat tail property. The results suggest CPI will follow its increasing trend. For cash rate, after the sudden spike, is predicted to drop quite soon. These forecasts suits the data properties well as CPI has a steady trend over the data window while cash rate is a fluctuating line."
  },
  {
    "objectID": "index.html#baseline-model-vs.-stochastic-volatility",
    "href": "index.html#baseline-model-vs.-stochastic-volatility",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "Baseline Model vs. Stochastic Volatility",
    "text": "Baseline Model vs. Stochastic Volatility\nThe BVAR-SV model forecasts deviate from the baseline by a significant margin, visually evident in the cash rate forecasts. It predict a much slower decrease than the other two models."
  },
  {
    "objectID": "index.html#forecasts-comparison",
    "href": "index.html#forecasts-comparison",
    "title": "Forecasting Inflation and Interest Rate in Australia",
    "section": "Forecasts Comparison",
    "text": "Forecasts Comparison\n\nCPI\nThere is only one quarterly data point available for comparison purpose. The factual and forecast values are presented and counter intuitively, the baseline model forecast is closest to true value, which is higher than all te forecast values. The reason behind this is possibly some abnormal economic activity after the COVID period, such as “greedflation”, where corporations ride the inflation trend to further raise the price. This is worthy of future investigation.\n\n\n\n\n\nData\nValue\n\n\n\n\nFactual\n137.4000\n\n\nBVAR\n137.3314\n\n\nBVAR-T\n137.3315\n\n\nBVAR-SV\n137.1805\n\n\n\n\n\n\n\nCash Rate\nThe figure clearly show that the BVAR-SV forecasts are the most accurate to reality."
  }
]